{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "## 课程问题\n",
    "* what do you want to acquire in this course？\n",
    "  \n",
    "  机器学习 & nlp 相关技术实践\n",
    "  \n",
    "* what problems do you want to solve？\n",
    "\n",
    "  智能问答，比如产品咨询，问题解答方面的机器人\n",
    "\n",
    "* what’s the advantages you have to finish you goal\n",
    "\n",
    "  较强的编程背景，对Python较为熟悉。\n",
    "  \n",
    "* what’s the disadvantages you need to overcome to finish you goal\n",
    "  \n",
    "  机器学习 & nlp 相关的数学基础，工程实践方法。\n",
    "  \n",
    "* How will you plan to study in this course period\n",
    "\n",
    "  1) 听课，完成作业\n",
    "  \n",
    "  2) 思考理解涉及的基础概念，和算法。\n",
    "  \n",
    "  3) 阅读其他相关的资料观看机器学习nlp相关的视频。\n",
    "  \n",
    "  4) 多练习一些难度适宜的小项目，通过练习加深理解。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mike\n",
      "老梁快请进！\n",
      "一个蓝色的小猫听着这个蓝色的好看的小猫\n",
      "if(d1bb){2b=2}\n"
     ]
    }
   ],
   "source": [
    "## 代码复现\n",
    "\n",
    "import random\n",
    "\n",
    "sentence_rules = '''\n",
    "say_hello = names hello tail \n",
    "names = name names | name\n",
    "name = Jhon | Mike | 老梁 | 老刘 \n",
    "hello = 你好 | 您来啦 | 快请进\n",
    "tail = 呀 | ！\n",
    "'''\n",
    "\n",
    "def generate_sentence(rules, target):\n",
    "    if target in rules:\n",
    "        target_rules = rules[target]\n",
    "        rule = random.choice(target_rules)\n",
    "        return ''.join(generate_sentence(rules, target=c.strip()) for c in rule.split())\n",
    "    \n",
    "    return target\n",
    "    \n",
    "sentence = generate_sentence({\n",
    "    'say_hello': ['names hello tail'],\n",
    "    'names': ['name names', 'name'],\n",
    "    'name': ['Jhon', 'Mike'],\n",
    "    'hello': ['你好', '您来啦'],\n",
    "    'tail': ['呀', '!']\n",
    "}, 'name')\n",
    "print(sentence)\n",
    "\n",
    "\n",
    "def get_generated_sentence(rules_str: str, target, stmt_sep='=', or_sep='|'):\n",
    "    rules = dict()\n",
    "    for line in rules_str.split('\\n'):\n",
    "        if not line:\n",
    "            continue\n",
    "        stmt, expr = line.split(stmt_sep)\n",
    "        rules[stmt.strip()] = expr.split(or_sep)\n",
    "        \n",
    "    return generate_sentence(rules, target=target)\n",
    "\n",
    "print(get_generated_sentence(sentence_rules, 'say_hello'))\n",
    "\n",
    "\n",
    "simple_grammar = \"\"\"\n",
    "sentence => noun_phrase verb_phrase\n",
    "noun_phrase => Article Adj* noun\n",
    "Adj* => Adj | Adj Adj*\n",
    "verb_phrase => verb noun_phrase\n",
    "Article =>  一个 | 这个\n",
    "noun =>   女人 |  篮球 | 桌子 | 小猫\n",
    "verb => 看着   |  坐在 |  听着 | 看见\n",
    "Adj =>   蓝色的 |  好看的 | 小小的\n",
    "\"\"\"\n",
    "print(get_generated_sentence(simple_grammar, 'sentence', stmt_sep='=>'))\n",
    "\n",
    "\n",
    "simpel_programming = '''\n",
    "if_stmt => if ( cond ) { stmt }\n",
    "cond => var op var\n",
    "op => | == | < | >= | <= \n",
    "stmt => assign | if_stmt\n",
    "assign => var = var\n",
    "var =>  char var | char\n",
    "char => a | b |  c | d | 0 | 1 | 2 | 3\n",
    "'''\n",
    "print(get_generated_sentence(simpel_programming, 'if_stmt', stmt_sep='=>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33425826\n"
     ]
    }
   ],
   "source": [
    "corpus = 'article_9k.txt'\n",
    "FILE = open(corpus).read()\n",
    "print(len(FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import jieba\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "max_length = 10 ** 7\n",
    "sub_file = FILE[:max_length]\n",
    "\n",
    "def cut(string):\n",
    "    return list(jieba.cut(string))\n",
    "\n",
    "TOKENS = cut(sub_file)\n",
    "print(type(TOKENS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'collections.Counter'>\n",
      "[('的', 241885), ('在', 70718), ('了', 46055), ('和', 42087), ('n', 40640), ('是', 38388), ('月', 36886), ('日', 28718), ('年', 27118), ('也', 19725), ('为', 19350), ('将', 19270), ('\\n', 19261), ('等', 19120), ('有', 18437), ('与', 17838), ('中', 17764), ('对', 17487), ('中国', 17037), ('新华社', 14988)]\n",
      "['此外自', '自本周', '本周6', '6月', '月12', '12日起', '日起除', '除小米', '小米手机', '手机6']\n",
      "1\n",
      "1\n",
      "0.0020161290322580645\n",
      "0.000275178866263071\n"
     ]
    }
   ],
   "source": [
    "words_count = Counter(TOKENS)\n",
    "print(type(words_count))\n",
    "\n",
    "most_20_commons = words_count.most_common(20)\n",
    "print(most_20_commons)\n",
    "\n",
    "_2_gram_words = [TOKENS[i]+TOKENS[i+1] for i in range(len(TOKENS)-1)]\n",
    "print(_2_gram_words[:10])\n",
    "\n",
    "_2_gram_words_count = Counter(_2_gram_words)\n",
    "\n",
    "def get_1_gram_words_count(word):\n",
    "    return words_count[word] if word in words_count else words_count.most_common()[-1][-1]\n",
    "\n",
    "def get_2_gram_words_count(word):\n",
    "    return _2_gram_words_count[word] if word in _2_gram_words_count else _2_gram_words_count.most_common()[-1][-1]\n",
    "\n",
    "def get_gram_count(word, wc):\n",
    "    return wc[word] if word in wc else wc.most_common()[-1][-1]\n",
    "\n",
    "print(get_gram_count('YYYY', words_count))\n",
    "print(get_gram_count('YYYY', _2_gram_words_count))\n",
    "\n",
    "def _2_gram_model(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    p = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        _2_gram_count = get_gram_count(word+next_word, _2_gram_words_count)\n",
    "        _1_gram_count = get_gram_count(next_word, words_count)\n",
    "        wp = _2_gram_count / _1_gram_count\n",
    "        \n",
    "        p = p * wp\n",
    "        \n",
    "    return p\n",
    "\n",
    "print(_2_gram_model('你好啊'))\n",
    "print(_2_gram_model('真好你'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论\n",
    "\n",
    "- 0. Can you come up out 3 sceneraies which use AI methods?\n",
    "  \n",
    "  1) 人脸识别\n",
    "  2) 语音识别\n",
    "  3) 智能客服\n",
    "  \n",
    "- 1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "\n",
    "   1) 我们用github可以存储代码提交历史，方便别人评审我们的代码\n",
    "   2) Jupyter 通过交互式的开发，可以让别人看到这个开发流程，获得及时反馈\n",
    "   3) Pycharm 通过这个python IDE 我们可以方便的开发，调式，查看源代码。\n",
    "   \n",
    "- 2. What's the Probability Model?\n",
    "   \n",
    "   基于概率的语言模型\n",
    "   \n",
    "- 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "\n",
    "   1) 语音识别。\n",
    "   2) 判断一句话是否符合人类的逻辑\n",
    "   \n",
    "- 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "\n",
    "    基于编程的处理当语言发生变化的时候，需要修改代码，而且容易出错。\n",
    "    \n",
    "    基于概率的处理更准确，不容易随着语言的变化，而改变。\n",
    "\n",
    "\n",
    "- 5. What's the Language Model;\n",
    "\n",
    "    一句话出现的概率。\n",
    "    \n",
    "- 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "     \n",
    "     1) 语音识别\n",
    "     2) 机器翻译\n",
    "     3) OCR, HWR\n",
    "    \n",
    "- 7. What's the 1-gram language model\n",
    "    \n",
    "    一个词语出现的概率仅和前一个单词有关。\n",
    "    \n",
    "- 8. What's the disadvantages and advantages of 1-gram language model\n",
    "\n",
    "    优势是表达式简单，容易实现，容易理解。\n",
    "    \n",
    "    劣势是不够精确\n",
    "\n",
    "\n",
    "- 9. What't the 2-gram models\n",
    "   \n",
    "   一个词语出现的概率和前面两个词语有关。\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他好吗\n",
      "她吃。\n",
      "['我知道吗', '她知道吗', '他好吗', '他吃了吗', '你知道吗', '她喜欢?', '你喜欢?', '你好吗', '我好吗', '他知道?']\n",
      "['他很喜欢了', '我很好了', '她不知道了', '她很喜欢。', '我很喜欢。', '她很喜欢。', '我很好啊', '我很好了', '他很喜欢。', '他不知道啊']\n"
     ]
    }
   ],
   "source": [
    "## 句子生成器\n",
    "\n",
    "ask_rule = '''\n",
    "wen_hou = 主语 谓语 宾语\n",
    "主语 = 你 | 我 | 他 | 她\n",
    "谓语 = 好 | 喜欢 | 吃了 | 快乐 | 知道\n",
    "宾语 = ? | 吗\n",
    "'''\n",
    "\n",
    "answer_rule = '''\n",
    "answer = 主语 谓语 宾语\n",
    "主语 = 我 | 他 | 她\n",
    "谓语 = 很好 | 好 | 很喜欢 | 不知道 | 吃\n",
    "宾语 = 啊 | 。| 了\n",
    "'''\n",
    "\n",
    "print(get_generated_sentence(ask_rule, 'wen_hou', stmt_sep='='))\n",
    "print(get_generated_sentence(answer_rule, 'answer', stmt_sep='='))\n",
    "\n",
    "def generate_n(grammar_rule, target, n, stmt_sep='=', or_sep='|'):\n",
    "    sentence = []\n",
    "    for _ in range(n):\n",
    "        sentence.append(get_generated_sentence(grammar_rule, target, stmt_sep, or_sep))\n",
    "        \n",
    "    return sentence\n",
    "\n",
    "print(generate_n(ask_rule, 'wen_hou', 10))\n",
    "print(generate_n(answer_rule, 'answer', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comments length:  261497\n",
      "吴京意淫到了脑残的地步，看了恶心想吐首映\n"
     ]
    }
   ],
   "source": [
    "## 2. 使用新数据源完成语言模型的训练\n",
    "\n",
    "import csv\n",
    "\n",
    "csv_file = 'movie_comments.csv'\n",
    "\n",
    "def get_comments(file_path):\n",
    "    comments = []\n",
    "    \n",
    "    with open(file_path, newline='\\n') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            cmt = row['comment']\n",
    "            comments.append(cmt.strip())\n",
    "    print('comments length: ', len(comments))        \n",
    "    return ''.join(comments)\n",
    "\n",
    "MOVIE_COMMENTS = get_comments(csv_file)\n",
    "print(MOVIE_COMMENTS[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00014382970562853582\n",
      "5.801810164771409e-05\n"
     ]
    }
   ],
   "source": [
    "MOVIE_COMMENTS_TOKENS = cut(MOVIE_COMMENTS)\n",
    "\n",
    "movie_comments_wc = Counter(MOVIE_COMMENTS_TOKENS)\n",
    "two_gram_comments = [MOVIE_COMMENTS_TOKENS[i]+MOVIE_COMMENTS_TOKENS[i+1] for i in range(len(MOVIE_COMMENTS_TOKENS)-1)]\n",
    "two_gram_comments_wc = Counter(two_gram_comments)\n",
    "\n",
    "def two_gram_model_v1(sentence):\n",
    "    tokens = cut(sentence)\n",
    "    p = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        word = tokens[i]\n",
    "        next_word = tokens[i+1]\n",
    "        \n",
    "        _2_gram_count = get_gram_count(word+next_word, two_gram_comments_wc)\n",
    "        _1_gram_count = get_gram_count(next_word, movie_comments_wc)\n",
    "        wp = _2_gram_count / _1_gram_count\n",
    "        \n",
    "        p = p * wp\n",
    "        \n",
    "    return p\n",
    "\n",
    "print(two_gram_model_v1('你好啊'))\n",
    "print(two_gram_model_v1('真好你'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('一个小小的女人看着这个小小的好看的小小的好看的好看的蓝色的蓝色的篮球', 4.2105349871042184e-43), ('这个蓝色的蓝色的小小的女人听着这个小小的好看的女人', 6.454936289320092e-32), ('一个蓝色的小小的桌子看见一个好看的好看的篮球', 4.7631621013464295e-30), ('一个蓝色的篮球看着这个蓝色的好看的小小的小猫', 2.0668204281416104e-29), ('这个蓝色的好看的桌子听着一个小小的好看的女人', 1.0040044564453749e-28), ('这个小小的篮球看着一个好看的好看的小小的篮球', 1.891484750453614e-28), ('一个好看的好看的篮球看见这个蓝色的篮球', 4.0364599250593788e-28), ('一个好看的蓝色的女人看见这个小小的蓝色的篮球', 2.0293472353323126e-27), ('一个好看的好看的女人看见一个蓝色的桌子', 1.9551222382349085e-26), ('这个蓝色的桌子看见这个好看的好看的女人', 6.8187675780265e-26), ('这个好看的小小的女人听着一个好看的篮球', 1.2822588301240749e-25), ('一个好看的小小的篮球看见一个蓝色的篮球', 1.886025783862267e-25), ('一个好看的小小的小小的小猫看着这个蓝色的小猫', 1.4843876943972048e-23), ('一个小小的小小的篮球看见这个蓝色的小小的桌子', 5.249672371799972e-21), ('这个好看的女人看见一个蓝色的桌子', 2.4373298272227078e-20), ('一个小小的好看的小小的小小的小猫看见一个小小的桌子', 1.0319327649501377e-19), ('一个小小的小小的女人坐在这个蓝色的小猫', 2.5977537639445313e-18), ('这个小小的女人坐在一个蓝色的女人', 6.846807815099163e-17), ('一个蓝色的小猫坐在一个小小的女人', 3.0166229726525135e-16), ('一个小小的小猫看着一个小小的女人', 2.5646349301758857e-11)]\n",
      "('一个小小的小猫看着一个小小的女人', 2.5646349301758857e-11)\n"
     ]
    }
   ],
   "source": [
    "## 3. 获得最优质的的语言\n",
    "\n",
    "## simple_grammar, 'sentence', stmt_sep='=>')\n",
    "def generate_best(grammar_rules, target, n, language_mode, stmt_sep='='):\n",
    "    sentence_list = generate_n(grammar_rules, target, n, stmt_sep=stmt_sep)\n",
    "    sentence_with_probability = [(words, language_mode(words)) for words in sentence_list]\n",
    "    sorted_sentence = sorted(sentence_with_probability, key=lambda x: x[1])\n",
    "    print(sorted_sentence)\n",
    "    return sorted_sentence[-1]\n",
    "\n",
    "# print(generate_best(simple_grammar, 'sentence', 10, two_gram_model_v1, stmt_sep='=>'))\n",
    "\n",
    "# ask_rule, 'wen_hou', stmt_sep='='\n",
    "# print(generate_best(ask_rule, 'wen_hou', 20, two_gram_model_v1))\n",
    "\n",
    "print(generate_best(simple_grammar, 'sentence', 20, _2_gram_model, stmt_sep='=>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
